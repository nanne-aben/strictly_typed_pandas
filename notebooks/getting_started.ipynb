{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd021955bae40816b58329a864495bd83642121ab031d49eff86d34b7b0569c6cea",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "source": [
    "## The problem\n",
    "\n",
    "I love Pandas! But in production code I’m always a bit wary when I see:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def foo(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # do stuff\n",
    "    return df"
   ]
  },
  {
   "source": [
    "Because… How do I know which columns are supposed to be in `df`?\n",
    "\n",
    "Sure, in a notebook this is often not a big problem, because we'll likely have\n",
    "* a few hundred lines of code\n",
    "* that you're working on alone\n",
    "* over a limited amount of time\n",
    "\n",
    "But what if this is production code, where we have:\n",
    "* \\>1000 lines of code\n",
    "* that we are maintaining for years to come\n",
    "* potentially by colleagues who haven't even been hired yet\n",
    "\n",
    "You'll probably want to be a bit more explicit about what these DataFrames should look like!\n",
    "\n",
    "## The solution: static type checking of pandas DataFrames\n",
    "\n",
    "Suppose we know that our DataFrame has two columns: `id` (an int) and `name` (a str). Using `strictly_typed_pandas`, we may write that down as follows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strictly_typed_pandas import DataSet\n",
    "\n",
    "class Schema:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "def foo(df: DataSet[Schema]) -> DataSet[Schema]:\n",
    "    # do stuff\n",
    "    return df"
   ]
  },
  {
   "source": [
    "These type definitions can now be checked using `mypy`, a linter for static type checking. The big benefit of `mypy` is that the type checking doesn't happen during run-time, but rather during linting time (so while you're coding), saving you precious time. If you haven't already, you should really check out how to set up `mypy` for your IDE.\n",
    "\n",
    "Let's consider an example of how this works. First, we'll create some data. Since `DataSet` is a subclass of `pd.DataFrame`, it has (nearly) all the functionality of a `DataFrame`, including:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id  name\n",
       "0   1  John\n",
       "1   2  Jane\n",
       "2   3  Jack"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>John</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>Jane</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Jack</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = DataSet[Schema]({\"id\": [1, 2, 3], \"name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "df"
   ]
  },
  {
   "source": [
    "We can now call `foo()` with our data. All types check out, so nothing special happens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = foo(df)"
   ]
  },
  {
   "source": [
    "However, if we instead try to run `foo()` on a `DataFrame`, mypy will throw the following error.\n",
    "\n",
    "(Shown as a comment here, but it will show up in your IDE if you set up mypy.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df)\n",
    "res = foo(df)\n",
    "# mypy(error): Argument 1 to \"foo\" has incompatible type \"DataFrame\"; expected \"DataSet[Schema]\""
   ]
  },
  {
   "source": [
    "Likewise, if we call `foo()` on a `DataSet` with an alternative schema, mypy will throw the following error."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlternativeSchema:\n",
    "    id: int\n",
    "    first_name: str\n",
    "\n",
    "df = DataSet[AlternativeSchema]({\"id\": [1, 2, 3], \"first_name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "try:\n",
    "    res = foo(df)\n",
    "    # mypy(error): Argument 1 to \"foo\" has incompatible type \"DataSet[AlternativeSchema]\"; expected \"DataSet[Schema]\"\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "source": [
    "## How can we be sure that a DataSet adheres to its schema?\n",
    "\n",
    "The above is great if everyone is meticulous in keeping the schema annotations correct and up-to-date. But shouldn't we be worried that these schema annotations get out of sync? For example:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schema:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "def foo() -> DataSet[Schema]:\n",
    "    return DataSet[Schema](\n",
    "        {\n",
    "            \"id\": [1, 2, 3],\n",
    "            \"name\": [\"John\", \"Jane\", \"Jack\"],\n",
    "            \"job\": \"Data Scientist\"\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "source": [
    "Fortunately, we have some extra precautions in place that prevent the above scenario:\n",
    "* The schema of the data is validated during the `DataSet` creation.\n",
    "* `DataSet` is immutable, so its schema cannot change due to inplace modifications.\n",
    "\n",
    "As we will see, this means that if your codebase (e.g. `foo()`) is unit tested, functions like the above will result in errors and hence they shouldn't make it to the master branch. As such, you will be able to trust the schema annotations in your code base.\n",
    "\n",
    "Let's have a look at these precautions in more detail. First, if the columns in the data do not correspond to the ones defined in the shema, we get a TypeError, for example:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Schema contains the following columns not present in data: {'name'}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = DataSet[Schema]({\"id\": [1, 2, 3]})\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "Similarly, if the types defined in the schema don't match the types in the data, we again get a `TypeError`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Column name is of type numpy.int64, but the schema suggests <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = DataSet[Schema]({\"id\": [1, 2, 3], \"name\": [1, 2, 3]})\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "Hence, when we successsfully create our `DataSet[Schema]`, we can be certain that it adheres to the schema. \n",
    "\n",
    "Of course, for this to work, we do need to make sure that the `DataSet`'s columns and datatypes cannot be changed after its creation. This brings us to our second point: \n",
    "* We made `DataSet` immutable, so its schema cannot change due to inplace modifications.\n",
    "\n",
    "To this end, we have disabled operations such as:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "To ensure that the DataSet adheres to its schema, you cannot perform inplace modifications. You can either use dataset.to_dataframe() to cast the DataSet to a DataFrame, or use operations that return a DataFrame, e.g. df = df.assign(...).\n"
     ]
    }
   ],
   "source": [
    "df = DataSet[Schema]({\"id\": [1, 2, 3], \"name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "ids = [\"1\", \"2\", \"3\"]\n",
    "try:\n",
    "    df[\"id\"] = ids\n",
    "    df.id = ids\n",
    "    df.loc[:,\"id\"] = ids\n",
    "    df.iloc[:,0] = ids\n",
    "    df.assign(id=ids, inplace=True)\n",
    "except NotImplementedError as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "When you do need to make changes to the schema, you can either cast the `DataSet` back to a `DataFrame`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_dataframe()"
   ]
  },
  {
   "source": [
    "Or you can perform the `assign()` in the following way, which also casts it to a `DataFrame`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(id=ids)\n",
    "assert type(df) == pd.DataFrame"
   ]
  },
  {
   "source": [
    "In practice, this often means that functions have the following sequence:\n",
    "\n",
    "1. The input is a `DataSet[SchemaA]`\n",
    "2. The data is converted to a `DataFrame` so changes can be made\n",
    "3. The output is cast to `DataSet[SchemaB]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchemaA:\n",
    "    name: str\n",
    "\n",
    "class SchemaB:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "df = DataSet[SchemaA]({\"name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "\n",
    "def foo(df: DataSet[SchemaA]) -> DataSet[SchemaB]:\n",
    "    n = df.shape[0]\n",
    "    ids = range(n)\n",
    "    new_df = df.assign(id=ids)\n",
    "    return DataSet[SchemaB](new_df)"
   ]
  },
  {
   "source": [
    "Or alternatively in the more compact version"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(data: DataSet[SchemaA]) -> DataSet[SchemaB]:\n",
    "    return (\n",
    "        df.assign(id=lambda df: range(df.shape[0]))\n",
    "        .pipe(DataSet[SchemaB])\n",
    "    )"
   ]
  },
  {
   "source": [
    "## What about functions that return `Any`?\n",
    "So far we've seen that we can strictly type check our pandas data using a combination of linting checks and runtime checks. So is there anything that we haven't covered yet? Well, it turns out there is. Consider the following example.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Schema:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "\n",
    "def foo() -> DataSet[Schema]:\n",
    "    return (\n",
    "        DataSet[Schema]({\"id\": [1, 2, 3], \"name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "        .assign(job=\"Data Scientist\")\n",
    "        .iloc[:3]\n",
    "    )\n",
    "\n",
    "res = foo()"
   ]
  },
  {
   "source": [
    "Now this is interesting: `foo()` clearly returns something that doesn't adhere to the schema, but the above gives neither a linting error nor a runtime error!\n",
    "\n",
    "It turns out that the above problem often happens with functions like `iloc`, `loc` and `pipe`, whose return type is `Any` (and when you think about it, these can indeed return any possible datatype). When mypy sees that the return type is `Any`, it reasons that that could still be a `DataSet[Schema]` object, so it doesn't raise an error. It's only during runtime that we find out here that the return type actually is a `DataFrame`, but `mypy` doesn't do any runtime checks.\n",
    "\n",
    "Fortunately, Python offers other ways to do type checking during runtime. Here, we will use the `typeguard` package. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Type of the return value must be a DataSet[__main__.Schema]; got pandas.core.frame.DataFrame instead\n"
     ]
    }
   ],
   "source": [
    "from typeguard import typechecked\n",
    "\n",
    "@typechecked\n",
    "def foo() -> DataSet[Schema]:\n",
    "    return (\n",
    "        DataSet[Schema]({\"id\": [1, 2, 3], \"name\": [\"John\", \"Jane\", \"Jack\"]})\n",
    "        .assign(job=\"Data Scientist\")\n",
    "        .iloc[:3]\n",
    "    )\n",
    "\n",
    "try:\n",
    "    res = foo()\n",
    "except TypeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "Alright, we now caught the error dead in its tracks! \n",
    "\n",
    "We can improve this with one more step: instead of adding the `@typechecked` decorator to every function by hand (which could be error prone), `typeguard` can do this automatically when running the unit tests. To do this, simply run your unit tests using `pytest --typeguard-packages=foo.bar` (where `foo.bar` is your package name)\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "We can statically type check pandas in the following way:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strictly_typed_pandas import DataSet\n",
    "\n",
    "class Schema:\n",
    "    id: int\n",
    "    name: str\n",
    "\n",
    "def foo(df: DataSet[Schema]) -> DataSet[Schema]:\n",
    "    # do stuff\n",
    "    return df"
   ]
  },
  {
   "source": [
    "Where `DataSet`:\n",
    "* is a subclass of `pd.DataFrame` and hence has the same functionality as `DataFrame`.\n",
    "* validates whether the data adheres to the provided schema upon its initialization.\n",
    "* is immutable, so its schema cannot be changed using inplace modifications.\n",
    "\n",
    "The `DataSet[Schema]` annotations are compatible with:\n",
    "* `mypy` for type checking during linting-time (i.e. while you write your code).\n",
    "* `typeguard` for type checking during run-time (i.e. while you run your unit tests).\n",
    "\n",
    "To get the most out of `strictly_typed_pandas`, be sure to:\n",
    "* set up `mypy` in your IDE.\n",
    "* run your unit tests with `pytest --typeguard-packages=foo.bar` (where `foo.bar` is your package name)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}